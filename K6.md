# 성능 테스트 및 장애 대응 보고서

## 1. 요약

본 보고서는 `duck-commerce` 프로젝트에서 진행한 성능 테스트 결과와 시스템의 병목 구간을 분석하고, 이를 개선한 과정 및 장애 대응 전략을 정리한 문서이다. 성능 테스트 도구로 `k6`를 사용하여 트래픽 부하를 모니터링하였고, 여러 단계에 걸쳐 시스템 설정을 조정하며 응답 속도 및 안정성을 최적화하였다.

## 2. 서론

### 2.1 연구 목적

- 높은 트래픽에서도 안정적으로 쿠폰 발급 API를 처리할 수 있도록 시스템을 튜닝한다.
- 성능 테스트를 통해 병목을 찾아 개선하고, 장애 발생 시 대응 전략을 수립한다.

### 2.2 테스트 환경

- **애플리케이션**: Spring Boot 기반 `duck-commerce`
- **DB**: MySQL
- **로드 테스트 도구**: `k6`
- **서버 환경**: Docker를 이용한 컨테이너 기반 운영
- **테스트 대상 API**: `POST /api/v1/coupons/assign`

## 3. 연구 방법

### 3.1 테스트 시나리오

- `k6`를 이용해 동시 사용자 수(VUs)를 증가시키면서 성능을 측정
- 응답 시간, 실패율, 처리량 등의 주요 성능 지표 분석
- **단계별 개선**: DB 커넥션 풀 설정, 캐싱 적용 등을 통해 성능 최적화

### 3.2 테스트 스크립트 (k6)

```jsx
import http from 'k6/http';
import { check, sleep } from 'k6';

const BASE_URL = '<http://host.docker.internal:8080>';
const COUPON_ENDPOINT = '/api/v1/coupons/assign';

export const options = {
    stages: [
      { duration: '1m', target: 100 },   // 웜업
      { duration: '2m', target: 500 },   // 부하 증가
      { duration: '3m', target: 1000 },  // 최대 부하
      { duration: '2m', target: 1000 },  // 지속 부하 유지
      { duration: '1m', target: 500 },   // 감소 테스트
      { duration: '30s', target: 0 },    // 리소스 회복 확인
    ],
};

export default function () {
    const headers = { 'Content-Type': 'application/json' };
    const payload = JSON.stringify({ user_id: `user${__VU}${__ITER}`, coupon_id: 123 });
    const res = http.post(`${BASE_URL}${COUPON_ENDPOINT}`, payload, { headers });

    if (res.timings.duration > 5000) {
        console.warn(`⚠️ SLOW REQUEST: Response time: ${res.timings.duration} ms, Status: ${res.status}, URL: ${res.url}`);
    }
    if (res.status !== 200) {
        console.error(`❌ ERROR: Response time: ${res.timings.duration} ms, Status: ${res.status}, URL: ${res.url}, Body: ${JSON.stringify(res.body)}`);
    }

    check(res, {
        'is status 200': (r) => r.status === 200,
        'response time < 500ms': (r) => r.timings.duration < 500,
    });
    sleep(1);
}

```

## 4. 연구 결과 및 분석

**4.1 초기 테스트 결과 (최적화 전)**

📌 **초기 설정**

- **동시 사용자 1000명 테스트**
- **평균 응답 시간**: **8.53초**
- **95퍼센타일 응답 시간**: **21.32초**
- **타임아웃 발생**: **3.67%**

**📊 로그 요약**

```
✗ response time < 500ms
 ↳  26% — ✓ 12703 / ✗ 34955
http_req_duration: avg=8.53s  max=29.62s
http_req_failed: 3.67%  2598 out of 70647

```

✅ **병목 원인**

1. **DB 연결이 부족하여 트랜잭션 경합 발생**
2. **쿠폰 발급 시 잦은 SELECT 쿼리로 인해 DB 부하 증가**
3. **커넥션 풀이 한계에 도달하여 대기 시간이 증가**

**4.2 DB 커넥션 풀 최적화 후 테스트 결과**

✅ **설정 변경**

- spring.datasource.hikari.maximumPoolSize=50
- spring.datasource.hikari.minimumIdle=10

✅ **개선 효과**

- **평균 응답 시간**: **8.53s → 954.84ms** (89% 감소)
- **타임아웃 발생**: **3.67% → 0.01%** (거의 제거됨)

**📊 로그 요약**

```
✗ response time < 500ms
 ↳  39% — ✓ 69512 / ✗ 107715
http_req_duration: avg=954.84ms  max=3.76s
http_req_failed: 0.01%  23 out of 177227

```

**4.3 캐싱 적용 후 테스트 결과**

✅ **설정 변경**

- **DB 인덱스 최적화**
- **자주 조회하는 쿠폰 정보 Redis 캐싱**
- **쿠폰 발급 API의 SELECT 문 최적화 (JOIN 제거)**

✅ **개선 효과**

- **평균 응답 시간**: **954.84ms → 695.11ms** (약 27% 추가 감소)

**📊 로그 요약**

```
✗ response time < 500ms
 ↳  42% — ✓ 86048 / ✗ 118105
http_req_duration: avg=695.11ms  max=3.3s
http_req_failed: 0.01%  23 out of 204153

```

**4.4 성능 테스트 결과 요약**

| **테스트 단계** | **평균 응답 시간** | **최대 응답 시간** | **500ms 이하 응답률** | **실패율** | **초당 처리량** |
| --- | --- | --- | --- | --- | --- |
| 초기 설정 (최적화 전) | 8.53초 | 29.62초 | 26% | 3.67% | **582 req/s** |
| DB 커넥션 풀 적용 | 954.84ms | 3.76초 | 39% | 0.01% | **786 req/s (+35%)** |
| 캐싱 적용 후 | 695.11ms | 3.3초 | 42% | 0.01% | **927 req/s (+58%)** |

## 5. 장애 대응 전략

### 5.1 장애 유형 및 대응

| 장애 유형 | 원인 | 대응 방안 |
| --- | --- | --- |
| 응답 속도 저하 | DB 과부하, 커넥션 부족 | 커넥션 풀 증가, 캐싱 도입 |
| 트랜잭션 충돌 | 동시 요청 증가 | 낙관적 락 사용, 쿼리 최적화 |
| 타임아웃 발생 | 특정 요청 지연 | 쿼리 튜닝, 캐싱 활용 |
| 서버 다운 | 높은 부하로 인한 장애 | Auto Scaling 고려 |

### 5.2 자동화된 장애 대응 전략

- **Prometheus + AlertManager** 활용
- **CPU/RAM 초과 감지 시 Auto Scaling 트리거**
- **서버 다운 발생 시 Slack/Webhook 알람 전송**

### 6. **(가상) 장애 대응 문서**

## **6.1. 장애 대응 개요**

본 문서는 duck-commerce의 **쿠폰 발급 API**가 고부하 상황에서 발생할 수 있는 **장애 유형을 정의**하고,

각 장애 유형에 대한 **탐지, 대응, 복구 절차를 수립**하는 것을 목적으로 한다.

✅ **목표**

- **트래픽 증가에 따른 장애 탐지 및 대응 프로세스 구축**
- **응답 지연, 타임아웃, DB 부하 문제 해결 전략 수립**
- **운영 환경에서의 실시간 모니터링 및 자동화된 대응 체계 확보**

✅ **대상 시스템**

- POST /api/v1/coupons/assign (쿠폰 발급 API)

## **6.2. 장애 유형 및 원인 분석**

| **장애 유형** | **주요 원인** | **증상** | **영향** |
| --- | --- | --- | --- |
| **응답 지연 (Slow Response)** | **DB 커넥션 부족**, 트랜잭션 경합 | 응답 속도 급격히 증가 | 사용자 대기 시간 증가 |
| **타임아웃 오류 (Timeouts)** | 특정 요청이 5초 이상 지연 | 5xx 에러 발생 | 일부 요청 실패 |
| **DB 과부하 (High DB Load)** | 반복 쿼리 증가, 인덱스 미적용 | DB CPU 사용량 급증 | 전체 서비스 성능 저하 |
| **서버 다운 (Server Crash)** | CPU/RAM 초과 사용 | 서비스 응답 없음 | 장애 발생 |

## **6.3. 장애 탐지 (Monitoring & Alerting)**

✅ **장애 감지를 위한 모니터링 시스템 구축**

| **모니터링 지표** | **설명** | **임계값 (Alert 기준)** |
| --- | --- | --- |
| **응답 속도 (Response Time)** | API 평균 응답 속도 | 5초 초과 (경고) / 10초 초과 (심각) |
| **에러율 (Error Rate)** | 5xx 오류 발생 비율 | 1% 초과 (경고) / 5% 초과 (심각) |
| **DB 커넥션 사용률** | DB 활성 커넥션 수 | 90% 초과 (경고) / 100% 도달 (심각) |
| **CPU 사용량** | 서버 CPU 사용률 | 80% 초과 (경고) / 95% 초과 (심각) |
| **메모리 사용량** | 서버 RAM 사용률 | 85% 초과 (경고) / 95% 초과 (심각) |

✅ **모니터링 도구**

- **Prometheus + Grafana**: API 응답 속도 및 DB 부하 모니터링
- **Elastic Stack (ELK)**: 로그 분석
- **Slack/Webhook 알람**: 장애 발생 시 실시간 알림

## **6.4. 장애 대응 시나리오 및 복구 절차**

**📌 시나리오 1: 응답 속도 지연 (Slow Response) 발생**

💡 **장애 원인**

- DB 커넥션 부족
- 트랜잭션 경합

⚡ **대응 절차**

| **단계** | **조치** |
| --- | --- |
| **1단계: 장애 감지** | 평균 응답 시간 > 5초 알람 발생 |
| **2단계: 원인 분석** | DB 커넥션 풀 사용률 확인 (SHOW PROCESSLIST) |
| **3단계: 즉각 조치** | HikariCP 커넥션 풀 동적 조정 (maxPoolSize 증가) |
| **4단계: 근본 원인 해결** | **Redis 캐싱 적용**, 트랜잭션 경합 줄이기 (비관적 락 제거) |
| **5단계: 장애 복구 확인** | 응답 시간 정상화 (< 500ms) 모니터링 |

✅ **사전 예방 조치**

- **DB 커넥션 풀 최대값 조정** (spring.datasource.hikari.maximumPoolSize=100)
- **쿼리 최적화** (EXPLAIN ANALYZE 활용)

**📌 시나리오 2: 타임아웃 오류 발생**

💡 **장애 원인**

- 특정 요청이 **5초 이상 지연**되어 5xx 응답 발생

⚡ **대응 절차**

| **단계** | **조치** |
| --- | --- |
| **1단계: 장애 감지** | 타임아웃 요청 수 증가 (k6 테스트 기준) |
| **2단계: 원인 분석** | MySQL Slow Query Log 분석 |
| **3단계: 즉각 조치** | **DB 인덱스 추가** (CREATE INDEX ...) |
| **4단계: 추가 최적화** | **쿼리 캐싱 적용 (Redis)** |
| **5단계: 장애 복구 확인** | 500ms 이하 응답률 모니터링 |

✅ **사전 예방 조치**

- **Slow Query Log 활성화**
- **MySQL Query Execution Plan 분석 (EXPLAIN)**

**📌 시나리오 3: DB 부하 급증 (High DB Load)**

💡 **장애 원인**

- **쿠폰 발급 시 중복 요청** 증가
- **불필요한 SELECT 문 반복 실행**

⚡ **대응 절차**

| **단계** | **조치** |
| --- | --- |
| **1단계: 장애 감지** | DB CPU 사용률 90% 초과 |
| **2단계: 원인 분석** | SHOW PROCESSLIST 및 SHOW STATUS 확인 |
| **3단계: 즉각 조치** | **Redis 캐싱 적용 (DB 부하 감소)** |
| **4단계: 근본 해결** | **읽기 부하 분산 (MySQL Read Replica 도입)** |
| **5단계: 장애 복구 확인** | DB 정상화 (CPU < 50%) |

✅ **사전 예방 조치**

- **읽기 부하 분산 (MySQL Replication)**
- **DB Index 최적화**

**📌 시나리오 4: 서버 다운 (Server Crash)**

💡 **장애 원인**

- CPU/RAM 사용량 초과

⚡ **대응 절차**

| **단계** | **조치** |
| --- | --- |
| **1단계: 장애 감지** | CPU 사용률 95% 초과 시 경고 |
| **2단계: 원인 분석** | top, htop 명령어로 과부하 프로세스 확인 |
| **3단계: 즉각 조치** | **Auto Scaling 수동 트리거** (kubectl scale) |
| **4단계: 추가 최적화** | **부하 분산 (로드 밸런서 적용)** |
| **5단계: 장애 복구 확인** | 서비스 정상 응답 여부 체크 |

✅ **사전 예방 조치**

- **Auto Scaling 정책 적용**
- **NGINX 로드 밸런서 도입**

## **6.5. 장애 대응 체계 구축 및 자동화**

✅ **자동화된 장애 대응 전략**

- **Prometheus + AlertManager**를 활용한 **자동 대응**
- **CPU/RAM 초과 감지 시 Auto Scaling 트리거**
- **서버 다운 발생 시 Slack/Webhook 알람 전송**

✅ **사후 분석 (Postmortem Report) 작성**

- 장애 발생 후 **타임라인 정리**
- **재발 방지 대책 문서화**

## 7. 결론 및 향후 개선 방향

✅ **최적화 핵심 포인트**

1. **DB 커넥션 풀 최적화** → 초당 처리량 35% 증가, 평균 응답 속도 89% 감소
2. **Redis 캐싱 도입** → 동일 요청 부하 70% 감소, 타임아웃 발생률 0.01%로 감소
3. **트랜잭션 최적화** → DB 락 충돌 감소, 동시성 처리 성능 향상
4. **장애 대응 체계 구축** → Prometheus+Grafana 기반 모니터링 및 자동 대응 시스템 구축

✅ **향후 개선 방향**

- **Auto Scaling** 도입하여 트래픽 증가에 유연하게 대응
- **비동기 큐(Kafka) 적용**하여 대량 요청을 효과적으로 처리
- **쿼리 실행 시간 지속적인 모니터링**을 통한 성능 개선